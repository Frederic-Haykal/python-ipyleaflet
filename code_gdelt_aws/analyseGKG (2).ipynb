{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce8aa997ed943178aeb064743c876f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1611268551592_0001</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-47-202.ec2.internal:20888/proxy/application_1611268551592_0001/\" >Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-35-137.ec2.internal:8042/node/containerlogs/container_1611268551592_0001_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//Janvier : V, #Frevrier: X Zlib, #Mars :X Zlib , Avril : X, Mai:X, juin, juillet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39d6dd7134e4af9a8305b3af49897ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import scala.sys.process._\n",
      "import java.net.URL\n",
      "import java.io.File\n",
      "import java.io.File\n",
      "import java.nio.file.{Files, StandardCopyOption}\n",
      "import java.net.HttpURLConnection\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.functions.udf\n",
      "import org.apache.spark.sql.{Row, SparkSession}\n",
      "import org.apache.spark.sql.functions.{col, to_date, to_timestamp}\n",
      "import org.apache.spark.sql.types.DateType\n",
      "import java.util.zip.ZipInputStream\n",
      "import java.io.BufferedReader\n",
      "import java.io.InputStreamReader\n",
      "import org.apache.spark.sql.SQLContext\n",
      "import com.amazonaws.services.s3.AmazonS3Client\n",
      "import org.apache.spark.sql.functions.{from_unixtime, unix_timestamp, _}\n",
      "import org.apache.spark.sql.types.{IntegerType, StringType, StructType}\n",
      "import org.apache.spark.sql.SaveMode\n",
      "import org.apache.spark.input.PortableDataStream\n",
      "import com.amazonaws.auth.BasicAWSCredentials\n",
      "import org.apache.spark.sql.types.IntegerType\n",
      "import com.amazonaws.services.s3.{AmazonS3, AmazonS3ClientBuilder}\n"
     ]
    }
   ],
   "source": [
    "// Imports\n",
    "import scala.sys.process._\n",
    "import java.net.URL\n",
    "import java.io.File\n",
    "import java.io.File\n",
    "import java.nio.file.{Files, StandardCopyOption}\n",
    "import java.net.HttpURLConnection \n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "//import scala.sqlContext.implicits._\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import org.apache.spark.sql.{Row, SparkSession}\n",
    "import org.apache.spark.sql.functions.{col, to_date, to_timestamp}\n",
    "import org.apache.spark.sql.types.DateType\n",
    "import java.util.zip.ZipInputStream\n",
    "import java.io.BufferedReader\n",
    "import java.io.InputStreamReader\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import com.amazonaws.services.s3.AmazonS3Client\n",
    "import org.apache.spark.sql.functions.{from_unixtime, unix_timestamp, _}\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructType}\n",
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "import org.apache.spark.input.PortableDataStream\n",
    "import com.amazonaws.auth.BasicAWSCredentials\n",
    "\n",
    "//import org.apache.spark.sql.cassandra._\n",
    "\n",
    "//import com.datastax.spark.connector._\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "\n",
    "\n",
    "import com.amazonaws.services.s3.{AmazonS3, AmazonS3ClientBuilder}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418a457de2fa4bb68229767cf7dd536c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9</td><td>application_1611268551592_0010</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-47-202.ec2.internal:20888/proxy/application_1611268551592_0010/\" >Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-35-137.ec2.internal:8042/node/containerlogs/container_1611268551592_0010_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list_csv_translate: org.apache.spark.sql.DataFrame = [size: string, hash: string ... 1 more field]\n",
      "list_2020_translate_tot: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [size: string, hash: string ... 1 more field]\n",
      "+-------+--------------------------------+---------------------------------------------------------------------------+\n",
      "|size   |hash                            |url                                                                        |\n",
      "+-------+--------------------------------+---------------------------------------------------------------------------+\n",
      "|6178636|72d0e5ca10362beca19a0510854f4721|http://data.gdeltproject.org/gdeltv2/20200201000000.translation.gkg.csv.zip|\n",
      "|6133442|765b9d4a4dd3270732defd66de74be37|http://data.gdeltproject.org/gdeltv2/20200201001500.translation.gkg.csv.zip|\n",
      "|6454334|209299355a96b7e5a9ecf3ec77d2f6de|http://data.gdeltproject.org/gdeltv2/20200201003000.translation.gkg.csv.zip|\n",
      "|5654513|1ddd9159fce3a3ebf01be064bdbc4bfa|http://data.gdeltproject.org/gdeltv2/20200201004500.translation.gkg.csv.zip|\n",
      "|5517145|5570a7f6cf093ec26464d07d0d849846|http://data.gdeltproject.org/gdeltv2/20200201010000.translation.gkg.csv.zip|\n",
      "|5602121|2c41db3dc0ddb82d8f0fd034baca89bd|http://data.gdeltproject.org/gdeltv2/20200201011500.translation.gkg.csv.zip|\n",
      "|6049830|22430c0bf433ec91e4fccb777d1f431e|http://data.gdeltproject.org/gdeltv2/20200201013000.translation.gkg.csv.zip|\n",
      "|6369154|39a46116c40134a5db96beeab30f434c|http://data.gdeltproject.org/gdeltv2/20200201014500.translation.gkg.csv.zip|\n",
      "|5541150|cbb2ef523e91f05a31f1f51f9fb8b516|http://data.gdeltproject.org/gdeltv2/20200201020000.translation.gkg.csv.zip|\n",
      "|4966093|8d8112546a147bc3a9d1322176ab3f0e|http://data.gdeltproject.org/gdeltv2/20200201021500.translation.gkg.csv.zip|\n",
      "|5464143|35e3d8448dd0079f049c686a6aafd06f|http://data.gdeltproject.org/gdeltv2/20200201023000.translation.gkg.csv.zip|\n",
      "|5997714|7587d659be5c8844f9819f3e86ee2706|http://data.gdeltproject.org/gdeltv2/20200201024500.translation.gkg.csv.zip|\n",
      "|5524668|e6d69c9bb0989902869cc2a975a1cadd|http://data.gdeltproject.org/gdeltv2/20200201030000.translation.gkg.csv.zip|\n",
      "|5202802|44523fb855f67ad40d5dc19b3f9963ee|http://data.gdeltproject.org/gdeltv2/20200201031500.translation.gkg.csv.zip|\n",
      "|4663946|3e06fe6d1fae36a72eeeafa3ad03a9a7|http://data.gdeltproject.org/gdeltv2/20200201033000.translation.gkg.csv.zip|\n",
      "|6223891|0680afd7745c352c8caaeb8fd3270fdd|http://data.gdeltproject.org/gdeltv2/20200201034500.translation.gkg.csv.zip|\n",
      "|6289553|e3973350ba403b22729a9ea6bb99bb58|http://data.gdeltproject.org/gdeltv2/20200201040000.translation.gkg.csv.zip|\n",
      "|5529083|ce4e34bd4cb4390c0db820afb8e4d682|http://data.gdeltproject.org/gdeltv2/20200201041500.translation.gkg.csv.zip|\n",
      "|6611102|ef1d87038dc248a9e5780978040c5fba|http://data.gdeltproject.org/gdeltv2/20200201043000.translation.gkg.csv.zip|\n",
      "|6332223|dc86490934ab044ccb1a3e8676b67bd1|http://data.gdeltproject.org/gdeltv2/20200201044500.translation.gkg.csv.zip|\n",
      "+-------+--------------------------------+---------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// English Data\n",
    "val list_csv_translate = spark.read.format(\"csv\").option(\"delimiter\", \" \").\n",
    "    csv(\"s3://testfuret/masterfilelist_translation.txt\").   \n",
    "    withColumnRenamed(\"_c0\",\"size\").\n",
    "    withColumnRenamed(\"_c1\",\"hash\").\n",
    "    withColumnRenamed(\"_c2\",\"url\")\n",
    "\n",
    "\n",
    "val list_2020_translate_tot = list_csv_translate.where(col(\"url\").like(\"%202002%.gkg.csv.zip\"))   \n",
    "list_2020_translate_tot.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36407dc8c1754f1fafc27feed5720160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gkgRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[15] at flatMap at <console>:57\n",
      "gkgDF: org.apache.spark.sql.DataFrame = [value: array<string>]\n",
      "dfGKG: org.apache.spark.sql.DataFrame = [GKGRECORDID: string, V2DATE: string ... 6 more fields]\n"
     ]
    }
   ],
   "source": [
    "val gkgRDD = sc.binaryFiles(\"s3://testfuret/Master_file /*202002*.gkg.csv.zip\",100).    \n",
    "   flatMap {  \n",
    "       \n",
    "       // decompresser les fichiers\n",
    "       case (name: String, content: PortableDataStream) =>\n",
    "\n",
    "          val zis = new ZipInputStream(content.open)\n",
    "          Stream.continually(zis.getNextEntry).\n",
    "                //takeWhile(_ != null).\n",
    "                takeWhile{\n",
    "                      case null => zis.closeEntry(); zis.close(); false\n",
    "                      case _ => true\n",
    "                     }.\n",
    "                flatMap { _ =>\n",
    "                   \n",
    "                    val br = new BufferedReader(new InputStreamReader(zis))\n",
    "                    Stream.continually(br.readLine()).takeWhile(_ != null)  \n",
    "                    \n",
    "                }\n",
    "         \n",
    "\n",
    "    }\n",
    "\n",
    "val gkgDF = gkgRDD.map(x => x.split(\"\\t\")).toDF()//.map(row => row.mkString(\";\")).map(x => x.split(\";\")).toDF()\n",
    "\n",
    "\n",
    "\n",
    "val dfGKG = gkgDF.withColumn(\"_tmp\", $\"value\").select(\n",
    "    $\"_tmp\".getItem(0).as(\"GKGRECORDID\"),\n",
    "    $\"_tmp\".getItem(1).as(\"V2DATE\"),\n",
    "    //$\"_tmp\".getItem(2).as(\"V2SOURCECOLLECTIONIDENTIFIER\"),\n",
    "    $\"_tmp\".getItem(3).as(\"V2SOURCECOMMONNAME\"),\n",
    "    $\"_tmp\".getItem(4).as(\"V2DOCUMENTIDENTIFIER\"),\n",
    "    //$\"_tmp\".getItem(5).as(\"V1COUNTS\"),\n",
    "    //$\"_tmp\".getItem(6).as(\"V2.1COUNTS\"),\n",
    "    $\"_tmp\".getItem(7).as(\"V1THEMES\"),\n",
    "    //$\"_tmp\".getItem(8).as(\"V2ENHANCEDTHEMES\"),\n",
    "    $\"_tmp\".getItem(9).as(\"V1LOCATIONS\"),\n",
    "    //$\"_tmp\".getItem(10).as(\"V2ENHANCEDLOCATIONS\"),\n",
    "    $\"_tmp\".getItem(11).as(\"V1PERSONS\"),\n",
    "    //$\"_tmp\".getItem(12).as(\"V2ENHANCEDPERSONS\"),\n",
    "    //$\"_tmp\".getItem(13).as(\"V1ORGANIZATIONS\"),\n",
    "    //$\"_tmp\".getItem(14).as(\"V2ENHANCEDORGANIZATIONS\"),\n",
    "    $\"_tmp\".getItem(15).as(\"V1TONE\")\n",
    "    //$\"_tmp\".getItem(16).as(\"V2.1ENHANCEDDATES\"),\n",
    "    //$\"_tmp\".getItem(17).as(\"V2GCAM\"),\n",
    "    //$\"_tmp\".getItem(18).as(\"V21SHARINGIMAGE\"),\n",
    "    //$\"_tmp\".getItem(19).as(\"V21RELATEDIMAGES\"),\n",
    "    //$\"_tmp\".getItem(20).as(\"V21SOCIALIMAGEEMBEDS\"),\n",
    "    //$\"_tmp\".getItem(21).as(\"V21SOCIALVIDEOEMBEDS\"),\n",
    "    //$\"_tmp\".getItem(22).as(\"V21QUOTATIONS\"),\n",
    "    //$\"_tmp\".getItem(23).as(\"V21ALLNAMES\"),\n",
    "    //$\"_tmp\".getItem(24).as(\"V21AMOUNTS\"),\n",
    "    //$\"_tmp\".getItem(25).as(\"V21TRANSLATIONINFO\"),\n",
    "    //$\"_tmp\".getItem(26).as(\"V2EXTRASXML\")\n",
    "    )\n",
    "\n",
    "//dfGKG.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c51133e1608414a9a6409d75612b812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------------------+--------------------+--------+-----------+---------+------+\n",
      "|GKGRECORDID|  V2DATE|V2SOURCECOMMONNAME|V2DOCUMENTIDENTIFIER|V1THEMES|V1LOCATIONS|V1PERSONS|V1TONE|\n",
      "+-----------+--------+------------------+--------------------+--------+-----------+---------+------+\n",
      "|  896047517|20190101|              2019|           2019.0027|        |           |         |   USA|\n",
      "|  896047518|20190101|              2019|           2019.0027|     CAN|           |         |   GBR|\n",
      "|  896047519|20190101|              2019|           2019.0027|        |           |         |   GBR|\n",
      "|  896047520|20190101|              2019|           2019.0027|     CYP|           |         |   GBR|\n",
      "|  896047521|20190101|              2019|           2019.0027|     ETH|           |         |      |\n",
      "|  896047522|20190101|              2019|           2019.0027|     ETH|           |         |      |\n",
      "|  896047523|20190101|              2019|           2019.0027|     GBR|           |         |      |\n",
      "|  896047524|20190101|              2019|           2019.0027|     GBR|           |         |   CAN|\n",
      "|  896047525|20190101|              2019|           2019.0027|     GBR|           |         |   CVL|\n",
      "|  896047526|20190101|              2019|           2019.0027|     GBR|           |         |   CVL|\n",
      "|  896047527|20190101|              2019|           2019.0027|     GBR|           |         |   ISR|\n",
      "|  896047528|20190101|              2019|           2019.0027|     JPN|           |         |   GOV|\n",
      "|  896047529|20190101|              2019|           2019.0027|     RUS|           |         |RUSGOV|\n",
      "|  896047530|20190101|              2019|           2019.0027|     RUS|           |         |   GOV|\n",
      "|  896047531|20190101|              2019|           2019.0027|     USA|           |         |      |\n",
      "|  896047532|20190101|              2019|           2019.0027|     USA|           |         |      |\n",
      "|  896047533|20191202|              2019|           2019.9096|        |           |         |      |\n",
      "|  896047534|20191202|              2019|           2019.9096|        |           |         |IRNGOV|\n",
      "|  896047535|20191202|              2019|           2019.9096|        |           |         |IRNGOV|\n",
      "|  896047536|20191225|              2019|           2019.9726|        |           |         |   JUD|\n",
      "+-----------+--------+------------------+--------------------+--------+-----------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfGKG.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fae8f766cc94a26836a481cbe26e485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mentionRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[23] at flatMap at <console>:57\n",
      "mentionDF: org.apache.spark.sql.DataFrame = [value: array<string>]\n",
      "dfMention: org.apache.spark.sql.DataFrame = [globaleventID: string, MentionSourceName: string ... 2 more fields]\n"
     ]
    }
   ],
   "source": [
    "val mentionRDD = sc.binaryFiles(\"s3://testfuret/Master_file /*202002*.mentions*\",100000).    \n",
    "   flatMap {  \n",
    "       \n",
    "       // decompresser les fichiers\n",
    "       case (name: String, content: PortableDataStream) =>\n",
    "\n",
    "          val zis = new ZipInputStream(content.open)\n",
    "          Stream.continually(zis.getNextEntry).\n",
    "                //takeWhile(_ != null).\n",
    "                takeWhile{\n",
    "                      case null => zis.closeEntry(); zis.close(); false\n",
    "                      case _ => true\n",
    "                     }.\n",
    "                flatMap { _ =>\n",
    "                   \n",
    "                    val br = new BufferedReader(new InputStreamReader(zis))\n",
    "                    Stream.continually(br.readLine()).takeWhile(_ != null)  \n",
    "                    \n",
    "                }\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val mentionDF = mentionRDD.map(x => x.split(\"\\t\")).map(row => row.mkString(\";\")).map(x => x.split(\";\")).toDF()\n",
    "\n",
    "\n",
    "\n",
    "val dfMention = mentionDF.withColumn(\"_tmp\", $\"value\").select(\n",
    "     $\"_tmp\".getItem(0).as(\"globaleventID\"),\n",
    "     //$\"_tmp\".getItem(1).as(\"EventTimeDate\"),\n",
    "     //$\"_tmp\".getItem(2).as(\"MentionTimeDate\"),\n",
    "     //$\"_tmp\".getItem(3).as(\"MentionType\"),\n",
    "     $\"_tmp\".getItem(4).as(\"MentionSourceName\"),\n",
    "     $\"_tmp\".getItem(5).as(\"MentionIdentifier\"),\n",
    "     //$\"_tmp\".getItem(6).as(\"SentenceID\"),\n",
    "     //$\"_tmp\".getItem(7).as(\"Actor1CharOffset\"),\n",
    "     //$\"_tmp\".getItem(8).as(\"Actor2CharOffset\"),\n",
    "     //$\"_tmp\".getItem(9).as(\"ActionCharOffset\"),\n",
    "     //$\"_tmp\".getItem(10).as(\"InRawText\"),\n",
    "     //$\"_tmp\".getItem(11).as(\"Confidence\"),\n",
    "     //$\"_tmp\".getItem(12).as(\"MentionDocLen\"),\n",
    "     //$\"_tmp\".getItem(13).as(\"MentionDocTone\"),\n",
    "     $\"_tmp\".getItem(14).as(\"MentionDocTranslationInfo\")\n",
    "     //$\"_tmp\".getItem(15).as(\"Extras\")\n",
    "     )\n",
    "\n",
    "//dfMention.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61348a2df3994c7692909274ce477a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EventRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[28] at flatMap at <console>:57\n",
      "eventDF: org.apache.spark.sql.DataFrame = [value: array<string>]\n",
      "dfEvent: org.apache.spark.sql.DataFrame = [globaleventID: string, day: string ... 7 more fields]\n"
     ]
    }
   ],
   "source": [
    "val EventRDD = sc.binaryFiles(\"s3://testfuret/Master_file /*202002*.export*\",100000).    \n",
    "   flatMap {  \n",
    "       \n",
    "       // decompresser les fichiers\n",
    "       case (name: String, content: PortableDataStream) =>\n",
    "\n",
    "          val zis = new ZipInputStream(content.open)\n",
    "          Stream.continually(zis.getNextEntry).\n",
    "                //takeWhile(_ != null).\n",
    "                takeWhile{\n",
    "                      case null => zis.closeEntry(); zis.close(); false\n",
    "                      case _ => true\n",
    "                     }.\n",
    "                flatMap { _ =>\n",
    "                   \n",
    "                    val br = new BufferedReader(new InputStreamReader(zis))\n",
    "                    Stream.continually(br.readLine()).takeWhile(_ != null)  \n",
    "                    \n",
    "                }\n",
    "        \n",
    "           \n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val eventDF = EventRDD.map(x => x.split(\"\\t\")).map(row => row.mkString(\";\")).map(x => x.split(\";\")).toDF()\n",
    "\n",
    "\n",
    "val dfEvent = eventDF.withColumn(\"_tmp\", $\"value\").select(\n",
    "     $\"_tmp\".getItem(0).as(\"globaleventID\"),\n",
    "     $\"_tmp\".getItem(1).as(\"day\"),\n",
    "     $\"_tmp\".getItem(2).as(\"month\"),\n",
    "     $\"_tmp\".getItem(3).as(\"year\"),\n",
    "     //$\"_tmp\".getItem(4).as(\"fractiondate\"),\n",
    "     //$\"_tmp\".getItem(5).as(\"Actor1code\"),\n",
    "     //$\"_tmp\".getItem(6).as(\"Actor1Name\"),\n",
    "     //$\"_tmp\".getItem(7).as(\"Actor1ContryCode\"),\n",
    "     //$\"_tmp\".getItem(8).as(\"actor1KnownGroupeCode\"),\n",
    "     //$\"_tmp\".getItem(9).as(\"Actor1EthnicCode\"),\n",
    "     //$\"_tmp\".getItem(10).as(\"Actor1Religion1Code\"),\n",
    "     //$\"_tmp\".getItem(11).as(\"Actor1Religion2code\"),\n",
    "     //$\"_tmp\".getItem(12).as(\"Actor1type1Code\"),\n",
    "     //$\"_tmp\".getItem(13).as(\"Actor1Type2code\"),\n",
    "     //$\"_tmp\".getItem(14).as(\"Actor1Type3code\"),\n",
    "     //$\"_tmp\".getItem(15).as(\"Actor2code\"),\n",
    "     //$\"_tmp\".getItem(16).as(\"Actor2Name\"),\n",
    "     //$\"_tmp\".getItem(17).as(\"Actor2ContryCode\"),\n",
    "     //$\"_tmp\".getItem(18).as(\"Actor2KnownGroupeCode\"),\n",
    "     //$\"_tmp\".getItem(19).as(\"Actor2EthnicCode\"),\n",
    "     //$\"_tmp\".getItem(20).as(\"Actor2Religion1Code\"),\n",
    "     //$\"_tmp\".getItem(21).as(\"Actor2Religion2code\"),\n",
    "     //$\"_tmp\".getItem(22).as(\"Actor2type1Code\"),\n",
    "     //$\"_tmp\".getItem(23).as(\"Actor2Type2code\"),\n",
    "     //$\"_tmp\".getItem(24).as(\"Actor2Type3code\"),\n",
    "     //$\"_tmp\".getItem(25).as(\"IsRootEvent\"),\n",
    "     //$\"_tmp\".getItem(26).as(\"EventCode\"),\n",
    "     //$\"_tmp\".getItem(27).as(\"EventBaseCode\"),\n",
    "     //$\"_tmp\".getItem(28).as(\"EventRootCode\"),\n",
    "     //$\"_tmp\".getItem(29).as(\"QuadClass\"),\n",
    "     //$\"_tmp\".getItem(30).as(\"GoldsteinScale\"),\n",
    "     $\"_tmp\".getItem(31).as(\"NumMentions\"),\n",
    "     $\"_tmp\".getItem(32).as(\"NumSources\"),\n",
    "     $\"_tmp\".getItem(33).as(\"NumArticles\"),\n",
    "     $\"_tmp\".getItem(34).as(\"AvgTone\"),\n",
    "     //$\"_tmp\".getItem(35).as(\"Actor1Geo_Type\"),\n",
    "     //$\"_tmp\".getItem(36).as(\"Actor1Geo_Fullname\"),\n",
    "     //$\"_tmp\".getItem(37).as(\"Actor1Geo_CountryCode\"),\n",
    "     //$\"_tmp\".getItem(38).as(\"Actor1Geo_ADM1Code\"),\n",
    "     //$\"_tmp\".getItem(39).as(\"Actor1Geo_ADM2Code\"),\n",
    "     //$\"_tmp\".getItem(40).as(\"Actor1Geo_Lat\"),\n",
    "     //$\"_tmp\".getItem(41).as(\"Actor1Geo_Long\"),\n",
    "     //$\"_tmp\".getItem(42).as(\"Actor1Geo_FeatureID\"),\n",
    "     //$\"_tmp\".getItem(43).as(\"Actor2Geo_Type\"),\n",
    "     //$\"_tmp\".getItem(44).as(\"Actor2Geo_Fullname\"),\n",
    "     //$\"_tmp\".getItem(45).as(\"Actor2Geo_CountryCode\"),\n",
    "     //$\"_tmp\".getItem(46).as(\"Actor2Geo_ADM1Code\"),\n",
    "     //$\"_tmp\".getItem(47).as(\"Actor2Geo_ADM2Code\"),\n",
    "     //$\"_tmp\".getItem(48).as(\"Actor2Geo_Lat\"),\n",
    "     //$\"_tmp\".getItem(49).as(\"Actor2Geo_Long\"),\n",
    "     //$\"_tmp\".getItem(50).as(\"Actor2Geo_FeatureID\"),\n",
    "     //$\"_tmp\".getItem(51).as(\"ActionGeo_Type\"),\n",
    "     $\"_tmp\".getItem(52).as(\"ActionGeo_Fullname\")\n",
    "     //$\"_tmp\".getItem(53).as(\"ActionGeo_CountryCode\"),\n",
    "     //$\"_tmp\".getItem(54).as(\"ActionGeo_ADM1Code\"),\n",
    "     //$\"_tmp\".getItem(55).as(\"ActionGeo_ADM2Code\"),\n",
    "     //$\"_tmp\".getItem(56).as(\"ActionGeo_Lat\"),\n",
    "     //$\"_tmp\".getItem(57).as(\"ActionGeo_Long\"),\n",
    "     //$\"_tmp\".getItem(58).as(\"ActionGeo_FeatureID\"),\n",
    "     //$\"_tmp\".getItem(59).as(\"DATEADDED\")\n",
    "    )\n",
    "\n",
    "\n",
    "//dfEvent.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEvent.count()\n",
    "//Janvier : 4705004, #Frevrier: X, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMention.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val dfGKG_Covid = dfGKG_Pandemic.union(dfGKG_Corona).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fred Maj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a2455e4f7f4206831c3d2c6bf9600d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res175: Long = 5088881\n"
     ]
    }
   ],
   "source": [
    "dfGKG.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312c4542cee74d41a61e117b62653517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfGKG_Pand: org.apache.spark.sql.DataFrame = [GKGRECORDID: string, V2DATE: string ... 7 more fields]\n",
      "dfGKG_Pandemic: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [GKGRECORDID: string, V2DATE: string ... 7 more fields]\n"
     ]
    }
   ],
   "source": [
    "val dfGKG_Pand = dfGKG.\n",
    "        withColumn(\"CheckCOVID\", $\"V1THEMES\".like(\"%PANDEMIC%\") || $\"V1THEMES\".like(\"%CORONAVIRUS%\"))\n",
    "\n",
    "val dfGKG_Pandemic = dfGKG_Pand.\n",
    "    filter((dfGKG_Pand(\"CheckCOVID\") === \"true\"))\n",
    "//dfGKG_Pandemic.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd52e86dec924d0f99595a1a93fb3260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res92: Long = 457035\n"
     ]
    }
   ],
   "source": [
    "dfGKG_Pandemic.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ba04f24a274ff4b17a8abb7c20382e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_gkg_mention_corona: org.apache.spark.sql.DataFrame = [globaleventID: string, MentionSourceName: string ... 11 more fields]\n"
     ]
    }
   ],
   "source": [
    "val df_gkg_mention_corona =dfMention.join(\n",
    "  dfGKG_Pandemic,\n",
    "  dfMention(\"MentionIdentifier\") <=> dfGKG_Pandemic(\"V2DOCUMENTIDENTIFIER\")\n",
    "  )\n",
    "\n",
    "\n",
    "// val df_gkg_mention_corona =dfMention.join(\n",
    "//   broadcast(dfGKG_Pandemic),\n",
    "//   dfMention(\"MentionIdentifier\") <=> dfGKG_Pandemic(\"V2DOCUMENTIDENTIFIER\")\n",
    "//   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMention.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dec6b5b666e4b3db4614aa81812195a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res229: Long = 1011840\n"
     ]
    }
   ],
   "source": [
    "df_gkg_mention_corona.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab26a524bbf141db92c2259d9a11f3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "join_Envent_Mention_Gkg_corrona: org.apache.spark.sql.DataFrame = [day: string, month: string ... 19 more fields]\n"
     ]
    }
   ],
   "source": [
    "//val join_Envent_Mention_Gkg_corrona = dfMention.join(dfEvent, dfMention(\"globaleventID\")===dfEvent(\"globaleventID\"), \"inner\").\n",
    "\n",
    "val join_Envent_Mention_Gkg_corrona =dfEvent.join(\n",
    "  df_gkg_mention_corona,\n",
    "  dfEvent(\"globaleventID\") <=> df_gkg_mention_corona(\"globaleventID\")\n",
    "  ).drop(dfEvent(\"globaleventID\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8f14a09e8a497f8f0c0770ce04ddb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----------+----------+-----------+-----------------+--------------------+-------------+--------------------+--------------------+-------------------------+-------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     day| month|year|NumMentions|NumSources|NumArticles|          AvgTone|  ActionGeo_Fullname|globaleventID|   MentionSourceName|   MentionIdentifier|MentionDocTranslationInfo|        GKGRECORDID|        V2DATE|  V2SOURCECOMMONNAME|V2DOCUMENTIDENTIFIER|            V1THEMES|         V1LOCATIONS|           V1PERSONS|              V1TONE|CheckCOVID|\n",
      "+--------+------+----+-----------+----------+-----------+-----------------+--------------------+-------------+--------------------+--------------------+-------------------------+-------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|19200101|192001|1920|          7|         3|          7|-1.33755530210403|Minnesota, United...|    896047809|          agweek.com|https://www.agwee...|                     null| 20200101000000-533|20200101000000|          agweek.com|https://www.agwee...|GENERAL_HEALTH;ME...|2#Minnesota, Unit...|joni scheftel;chr...|-0.97799511002444...|      true|\n",
      "|19200101|192001|1920|          7|         3|          7|-1.33755530210403|Minnesota, United...|    896047809|brainerddispatch.com|https://www.brain...|                     null| 20200101000000-399|20200101000000|brainerddispatch.com|https://www.brain...|TAX_DISEASE;TAX_D...|2#Minnesota, Unit...|joni scheftel;chr...|-1.60550458715596...|      true|\n",
      "|19200101|192001|1920|          7|         3|          7|-1.33755530210403|Minnesota, United...|    896047809|         inforum.com|https://www.infor...|                     null| 20200101023000-313|20200101023000|         inforum.com|https://www.infor...|TAX_DISEASE;TAX_D...|2#Minnesota, Unit...|joni scheftel;chr...|-1.59090909090909...|      true|\n",
      "|19200101|192001|1920|          1|         1|          1|-8.59301227573182|Connecticut, Unit...|    896050288|  nbcconnecticut.com|https://www.nbcco...|                     null|20200101001500-1219|20200101001500|  nbcconnecticut.com|https://www.nbcco...|TAX_FNCACT;TAX_FN...|2#Oklahoma, Unite...|norman pattis;ale...|-8.17610062893082...|      true|\n",
      "|19200101|192001|1920|          1|         1|          1|-8.59301227573182|Connecticut, Unit...|    896050288|     darientimes.com|https://www.darie...|                     null|  20200101191500-89|20200101191500|     darientimes.com|https://www.darie...|LEGISLATION;EPU_P...|1#United States#U...|robert frost;mizu...|-1.83361629881154...|      true|\n",
      "|19200101|192001|1920|          5|         1|          5|-6.40394088669951|New York, United ...|    896054462|           vdare.com|https://vdare.com...|                     null| 20200101091500-274|20200101091500|           vdare.com|https://vdare.com...|TAX_FNCACT;TAX_FN...|2#New York, Unite...|paul kersey;fredd...|-6.79304897314376...|      true|\n",
      "|19200101|192001|1920|          2|         1|          2|-1.50537634408602|Washington, Unite...|    896058620|            kake.com|http://www.kake.c...|                     null| 20200101004500-366|20200101004500|            kake.com|http://www.kake.c...|EDUCATION;MANMADE...|2#Florida, United...|mary gomez camba;...|-1.47679324894515...|      true|\n",
      "|19200101|192001|1920|          2|         1|          2|-2.16606498194946|Washington, Distr...|    896060396|      nzherald.co.nz|https://www.nzher...|                     null| 20200101204500-204|20200101204500|      nzherald.co.nz|https://www.nzher...|BAN;TAX_FNCACT;TA...|1#United States#U...|paul blair;matthe...|-2.17142857142857...|      true|\n",
      "|19200101|192001|1920|          2|         1|          2|-2.16606498194946|Washington, Distr...|    896060396|         thehill.com|https://thehill.c...|                     null| 20200101031500-284|20200101031500|         thehill.com|https://thehill.c...|BAN;ARMEDCONFLICT...|3#Washington, Was...|                    |-2.32558139534884...|      true|\n",
      "|19200101|192001|1920|          3|         1|          3|-0.16233766233766|New York, United ...|    896062453|          abc.net.au|https://www.abc.n...|                     null| 20200101013000-150|20200101013000|          abc.net.au|https://www.abc.n...|NATURAL_DISASTER;...|2#New York, Unite...|donald trump;vlad...|0,3.6450079239302...|      true|\n",
      "|19200101|192001|1920|         10|         1|         10| -2.1505376344086|       United States|    896062827|             bgr.com|https://bgr.com/2...|                     null| 20200101013000-238|20200101013000|             bgr.com|https://bgr.com/2...|TAX_DISEASE;TAX_D...|1#United States#U...|        tim robinson|-1.83150183150183...|      true|\n",
      "|19200101|192001|1920|          8|         2|          8|-5.20100011808841|Washington, Distr...|    896067165| heraldmailmedia.com|https://www.heral...|                     null|20200101020000-1102|20200101020000| heraldmailmedia.com|https://www.heral...|AFFECT;REFUGEES;E...|3#Sandy Hook, Tex...|donald trump;just...|-5.05902192242833...|      true|\n",
      "|19200101|192001|1920|          8|         2|          8|-5.20100011808841|Washington, Distr...|    896067165|         fltimes.com|https://www.fltim...|                     null|20200101020000-1008|20200101020000|         fltimes.com|https://www.fltim...|AFFECT;REFUGEES;E...|3#Sandy Hook, Tex...|donald trump;just...|-5.2128583840139,...|      true|\n",
      "|19200101|192001|1920|          2|         1|          2| 3.90879478827362|Washington, Distr...|    896068415|    freerepublic.com|http://freerepubl...|                     null| 20200101190000-919|20200101190000|    freerepublic.com|http://freerepubl...|TAX_ETHNICITY;TAX...|2#New York, Unite...|john semmens;paul...|-3.06030603060306...|      true|\n",
      "|19200101|192001|1920|          4|         1|          4|-6.52528548123981|Karachi, Sindh, P...|    896074048|            dawn.com|https://www.dawn....|                     null| 20200101033000-608|20200101033000|            dawn.com|https://www.dawn....|EPU_CATS_MIGRATIO...|5#Balochistan, Ba...|                    |-6.34674922600619...|      true|\n",
      "|19200101|192001|1920|          2|         1|          2|-2.17129071170085|Palm Beach, Flori...|    896075131|   palmbeachpost.com|https://www.palmb...|                     null|  20200101043000-18|20200101043000|   palmbeachpost.com|https://www.palmb...|BAN;TAX_FNCACT;TA...|3#Washington, Was...|alex azar;donald ...|-2.23529411764706...|      true|\n",
      "|19200101|192001|1920|          2|         1|          2|-2.17129071170085|Palm Beach, Flori...|    896075131|palmbeachdailynew...|https://www.palmb...|                     null| 20200101034500-352|20200101034500|palmbeachdailynew...|https://www.palmb...|BAN;TAX_FNCACT;TA...|3#Washington, Was...|alex azar;donald ...|-2.23529411764706...|      true|\n",
      "|19200101|192001|1920|          8|         1|          8|-1.78384050367261|Terre Haute, Indi...|    896078531|        tribstar.com|https://www.tribs...|                     null| 20200101043000-181|20200101043000|        tribstar.com|https://www.tribs...|NATURAL_DISASTER;...|2#Indiana, United...|john smithson;tod...|-1.60642570281124...|      true|\n",
      "|19200101|192001|1920|         10|         1|         10|-3.66492146596859|Lagos, Lagos, Nig...|    896083209|thenationonlineng...|https://thenation...|                     null| 20200101053000-441|20200101053000|thenationonlineng...|https://thenation...|WB_635_PUBLIC_HEA...|4#Lagos, Lagos, N...|tajudeen adebanjo...|-3.19148936170213...|      true|\n",
      "|19200101|192001|1920|          1|         1|          1|-1.36157337367625|Alausa, Nigeria (...|    896084289|       dailytimes.ng|https://dailytime...|                     null| 20200101054500-137|20200101054500|       dailytimes.ng|https://dailytime...|AGRICULTURE;TAX_F...|4#Lagos, Lagos, N...|adewale akeweje;s...|-1.38888888888889...|      true|\n",
      "+--------+------+----+-----------+----------+-----------+-----------------+--------------------+-------------+--------------------+--------------------+-------------------------+-------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_Envent_Mention_Gkg_corrona.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ca6741d81b46f7b5d09a5a63a7a6eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "org.apache.spark.SparkException: Job aborted.\n",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:174)\n",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:112)\n",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "  at org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)\n",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)\n",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:291)\n",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n",
      "  ... 58 elided\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 145, ip-172-31-35-137.ec2.internal, executor 3): java.io.EOFException: Unexpected end of ZLIB input stream\n",
      "\tat java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240)\n",
      "\tat java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)\n",
      "\tat java.util.zip.ZipInputStream.read(ZipInputStream.java:194)\n",
      "\tat sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "\tat java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "\tat java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "\tat $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:72)\n",
      "\tat $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:72)\n",
      "\tat scala.collection.immutable.Stream$.continually(Stream.scala:1279)\n",
      "\tat scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n",
      "\tat scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n",
      "\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n",
      "\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n",
      "\tat scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n",
      "\tat scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n",
      "\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n",
      "\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n",
      "\tat scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n",
      "\tat scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n",
      "\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n",
      "\tat scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n",
      "\tat scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n",
      "\tat scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n",
      "\tat scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)\n",
      "\tat scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)\n",
      "\tat scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:188)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2080)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2068)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2067)\n",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2067)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:988)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:988)\n",
      "  at scala.Option.foreach(Option.scala:257)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:988)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2301)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2250)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2239)\n",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:146)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$anonfun$finalPhysicalPlan$1.apply(AdaptiveSparkPlanExec.scala:174)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$anonfun$finalPhysicalPlan$1.apply(AdaptiveSparkPlanExec.scala:173)\n",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:778)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.finalPhysicalPlan(AdaptiveSparkPlanExec.scala:173)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:178)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:143)\n",
      "  ... 83 more\n",
      "Caused by: java.io.EOFException: Unexpected end of ZLIB input stream\n",
      "  at java.util.zip.InflaterInputStream.fill(InflaterInputStream.java:240)\n",
      "  at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:158)\n",
      "  at java.util.zip.ZipInputStream.read(ZipInputStream.java:194)\n",
      "  at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "  at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "  at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "  at java.io.InputStreamReader.read(InputStreamReader.java:184)\n",
      "  at java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "  at java.io.BufferedReader.readLine(BufferedReader.java:324)\n",
      "  at java.io.BufferedReader.readLine(BufferedReader.java:389)\n",
      "  at $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:72)\n",
      "  at $anonfun$1$$anonfun$apply$3$$anonfun$apply$4.apply(<console>:72)\n",
      "  at scala.collection.immutable.Stream$.continually(Stream.scala:1279)\n",
      "  at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n",
      "  at scala.collection.immutable.Stream$$anonfun$continually$1.apply(Stream.scala:1279)\n",
      "  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n",
      "  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n",
      "  at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n",
      "  at scala.collection.immutable.Stream$$anonfun$takeWhile$1.apply(Stream.scala:941)\n",
      "  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n",
      "  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n",
      "  at scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n",
      "  at scala.collection.immutable.Stream$$anonfun$append$1.apply(Stream.scala:255)\n",
      "  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)\n",
      "  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)\n",
      "  at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n",
      "  at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)\n",
      "  at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)\n",
      "  at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)\n",
      "  at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)\n",
      "  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "  at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:188)\n",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "  ... 3 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//join_Envent_Mention_Gkg_corrona.write.mode('append').json(\"s3://testfuret/result/Janvier_CSV/\")\n",
    "join_Envent_Mention_Gkg_corrona.write.format(\"org.apache.spark.sql.json\").mode(SaveMode.Append).save(\"s3://cgl2/fevrier/\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21df2eb25f95430d83da428d08508fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b43cdfb5cdb4875bba0dbe4579c4ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3e70252c06a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n//Save In Parquet\\njoin_Envent_Mention_Gkg_corrona.write.parquet(\"s3://testfuret/result/fevrier\")\\n//val df = spark.\\n//       read.\\n//       parquet(\"même chemin\")\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2379\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2380\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2381\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2382\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-116>\u001b[0m in \u001b[0;36mspark\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/exceptions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_errors_are_fatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/exceptions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions_to_handle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_errors_are_fatal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/sparkmagic/kernels/kernelmagics.py\u001b[0m in \u001b[0;36mspark\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mcoerce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coerce_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/sparkmagic/magics/sparkmagicsbase.py\u001b[0m in \u001b[0;36mexecute_spark\u001b[0;34m(self, cell, output_var, samplemethod, maxrows, samplefraction, session_name, coerce)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmimetype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_controller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown_session_on_spark_statement_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/sparkcontroller.py\u001b[0m in \u001b[0;36mrun_command\u001b[0;34m(self, command, client_name)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0msession_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session_by_name_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_to_use\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_sqlquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mstatement_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_statement_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             self._spark_events.emit_statement_execution_end_event(session.guid, session.kind, session.id,\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36m_get_statement_output\u001b[0;34m(self, session, statement_id)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFINAL_STATEMENT_STATUS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'progress'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0mretries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/notebook-env/lib/python3.7/site-packages/sparkmagic/livyclientlib/livysession.py\u001b[0m in \u001b[0;36msleep\u001b[0;34m(self, retries)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds_to_sleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# This function will refresh the status and get the logs in a single call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "//Save In Parquet\n",
    "join_Envent_Mention_Gkg_corrona.write.parquet(\"s3://testfuret/result/fevrier\")\n",
    "//val df = spark.\n",
    "//       read.\n",
    "//       parquet(\"même chemin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//dfGKG_Covid.write.mode(SaveMode.Overwrite).format(\"parquet\").saveAsTable(\"s3://testfuret/result/dfGKG_Corona.parquet\")\n",
    "//dfGKG_Covid.write.mode.save(\"s3://testfuret/result/dfGKG_Corona.parquet\", SaveMode.Append )\n",
    "dfGKG_Covid.write.format(\"parquet\").mode(\"append\").save(\"dfGKG_Covid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3811e44ce047a49ffbcd812219d2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "org.apache.spark.SparkException: Job aborted.\n",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\n",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:174)\n",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:114)\n",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:112)\n",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n",
      "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "  at org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)\n",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)\n",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:291)\n",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:249)\n",
      "  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:586)\n",
      "  ... 58 elided\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 363 tasks (1031.0 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2080)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2068)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2067)\n",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2067)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:988)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:988)\n",
      "  at scala.Option.foreach(Option.scala:257)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:988)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2301)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2250)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2239)\n",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:799)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeCollectIterator(SparkPlan.scala:348)\n",
      "  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.org$apache$spark$sql$execution$exchange$BroadcastExchangeExec$$doComputeRelation(BroadcastExchangeExec.scala:70)\n",
      "  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1.doCompute(BroadcastExchangeExec.scala:63)\n",
      "  at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1.doCompute(BroadcastExchangeExec.scala:59)\n",
      "  at org.apache.spark.sql.execution.AsyncDriverOperation$$anonfun$org$apache$spark$sql$execution$AsyncDriverOperation$$compute$1.apply(AsyncDriverOperation.scala:75)\n",
      "  at org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:171)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withExecutionId$1.apply(SQLExecution.scala:153)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:150)\n",
      "  at org.apache.spark.sql.execution.AsyncDriverOperation.org$apache$spark$sql$execution$AsyncDriverOperation$$compute(AsyncDriverOperation.scala:69)\n",
      "  at org.apache.spark.sql.execution.AsyncDriverOperation$$anonfun$computeFuture$1.apply$mcV$sp(AsyncDriverOperation.scala:51)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$$anon$1.run(SQLExecution.scala:224)\n",
      "  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "  at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "  at java.lang.Thread.run(Thread.java:748)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:146)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$anonfun$finalPhysicalPlan$1.apply(AdaptiveSparkPlanExec.scala:174)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$anonfun$finalPhysicalPlan$1.apply(AdaptiveSparkPlanExec.scala:173)\n",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:778)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.finalPhysicalPlan(AdaptiveSparkPlanExec.scala:173)\n",
      "  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:178)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:173)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:169)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:197)\n",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:194)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:169)\n",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:143)\n",
      "  ... 84 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//f_gkg_mention_corona.format(\"parquet\").mode(\"append\").save(\"dfGKG_Covid.parquet\")\n",
    "\n",
    "df_gkg_mention_corona.write.parquet(\"s3://testfuret/result/df_gkg_mention_corona.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.binaryFiles(\"s3://axel-michalewicz-telecom/masterFileTrans/mentions\",100).foreach{r=> {\n",
    "    val zis = new ZipInputStream(r.open)\n",
    "}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//dF.write.parquet(\"s3://testfuret/result/jk/\")\n",
    "//val df = spark.\n",
    "//       read.\n",
    "//       parquet(\"même chemin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
